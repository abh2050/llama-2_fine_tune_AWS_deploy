{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "49fc6f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # LLaMA-2 Fine-tuning with LoRA using Claude-style Instruction Data\n",
    "\n",
    "# This notebook demonstrates how to fine-tune a LLaMA-2 model using LoRA (Low-Rank Adaptation) with Claude-4 style instruction datasets. We'll cover:\n",
    "\n",
    "# 1. **Dataset Preparation**: Converting Claude-style conversations to LLaMA format\n",
    "# 2. **Model Setup**: Loading LLaMA-2 with LoRA configuration\n",
    "# 3. **Training**: Fine-tuning with optimized parameters\n",
    "# 4. **Evaluation**: Testing the fine-tuned model\n",
    "\n",
    "# ## Prerequisites\n",
    "# - CUDA-compatible GPU (recommended: 16GB+ VRAM)\n",
    "# - Hugging Face account with LLaMA-2 access\n",
    "# - Claude-style instruction dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a3bb1fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers datasets peft trl accelerate bitsandbytes wandb\n",
    "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006e4672",
   "metadata": {},
   "source": [
    "## üìö About the OpenAssistant-Guanaco Dataset\n",
    "\n",
    "We're using a **500-example subset** of the **timdettmers/openassistant-guanaco** dataset, which is a high-quality instruction-following dataset:\n",
    "\n",
    "### Dataset Features:\n",
    "- **Original Size**: ~10,000 high-quality conversations\n",
    "- **Used Size**: 500 examples (for optimal training speed)\n",
    "- **Format**: Human-Assistant dialogue pairs\n",
    "- **Quality**: Curated and filtered for instruction-following tasks\n",
    "- **Source**: Based on OpenAssistant conversations\n",
    "- **Preprocessing**: Already formatted with `### Human:` and `### Assistant:` markers\n",
    "\n",
    "### Why This Dataset Size:\n",
    "1. **Training Speed**: 500 examples provide fast training iterations\n",
    "2. **Quality over Quantity**: Still maintains high-quality instruction examples\n",
    "3. **Demonstration**: Perfect size for learning and experimentation\n",
    "4. **Resource Efficient**: Works well on limited computational resources\n",
    "5. **Good Performance**: Sufficient for effective fine-tuning\n",
    "\n",
    "### Training Benefits:\n",
    "- **Faster Iterations**: Complete training in minutes rather than hours\n",
    "- **Easy Experimentation**: Quick to test different hyperparameters\n",
    "- **Resource Friendly**: Lower memory and compute requirements\n",
    "- **Still Effective**: 500 high-quality examples can significantly improve model performance\n",
    "\n",
    "This subset provides an excellent balance between training speed and model quality improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d03f5f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: False\n",
      "CUDA device count: 0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    TrainingArguments,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from trl import SFTTrainer\n",
    "import wandb\n",
    "import os\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# Check if CUDA is available\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Current device: {torch.cuda.current_device()}\")\n",
    "    print(f\"Device name: {torch.cuda.get_device_name()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2e005257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading timdettmers/openassistant-guanaco dataset...\n",
      "This is a high-quality instruction-following dataset with ~10k conversations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset loaded successfully!\n",
      "Dataset structure: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 9846\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 518\n",
      "    })\n",
      "})\n",
      "Total examples: 9846\n",
      "\n",
      "Sample data structure:\n",
      "Keys: ['text']\n",
      "Sample text length: 1687\n",
      "First 200 characters: ### Human: Can you write a short introduction about the relevance of the term \"monopsony\" in economics? Please use examples related to potential monopsonies in the labour market and cite relevant rese...\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ Step 1: Load OpenAssistant-Guanaco Dataset\n",
    "\n",
    "print(\"Loading timdettmers/openassistant-guanaco dataset...\")\n",
    "print(\"This is a high-quality instruction-following dataset with ~10k conversations\")\n",
    "\n",
    "# Load the OpenAssistant-Guanaco dataset\n",
    "try:\n",
    "    # Load the dataset from Hugging Face Hub\n",
    "    guanaco_dataset = load_dataset(\"timdettmers/openassistant-guanaco\")\n",
    "    \n",
    "    print(f\"‚úÖ Dataset loaded successfully!\")\n",
    "    print(f\"Dataset structure: {guanaco_dataset}\")\n",
    "    \n",
    "    # The dataset has a 'train' split\n",
    "    raw_dataset = guanaco_dataset['train']\n",
    "    print(f\"Total examples: {len(raw_dataset)}\")\n",
    "    \n",
    "    # Show sample data structure\n",
    "    print(\"\\nSample data structure:\")\n",
    "    sample = raw_dataset[0]\n",
    "    print(f\"Keys: {list(sample.keys())}\")\n",
    "    print(f\"Sample text length: {len(sample['text'])}\")\n",
    "    print(f\"First 200 characters: {sample['text'][:200]}...\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading dataset: {e}\")\n",
    "    print(\"Falling back to sample data...\")\n",
    "    \n",
    "    # Fallback sample data\n",
    "    sample_data = [\n",
    "        {\"text\": \"### Human: What is quantum entanglement?\\n\\n### Assistant: Quantum entanglement is a phenomenon in quantum mechanics where two or more particles become correlated in such a way that the quantum state of each particle cannot be described independently, even when separated by large distances.\"},\n",
    "        {\"text\": \"### Human: How do I write a Python function to calculate factorial?\\n\\n### Assistant: Here's a simple recursive approach:\\n\\n```python\\ndef factorial(n):\\n    if n == 0 or n == 1:\\n        return 1\\n    return n * factorial(n - 1)\\n```\"},\n",
    "        {\"text\": \"### Human: Explain machine learning in simple terms.\\n\\n### Assistant: Machine learning is like teaching a computer to recognize patterns and make predictions, similar to how humans learn from experience.\"}\n",
    "    ]\n",
    "    \n",
    "    raw_dataset = Dataset.from_list(sample_data)\n",
    "    print(f\"Using fallback dataset with {len(raw_dataset)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3b288fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing OpenAssistant-Guanaco dataset...\n",
      "Original dataset size: 9846\n",
      "Filtered dataset size: 7617\n",
      "Final processed dataset size: 7617\n",
      "\n",
      "=== Sample Processed Examples ===\n",
      "\n",
      "Example 1:\n",
      "### Human: Can you write a short introduction about the relevance of the term \"monopsony\" in economics? Please use examples related to potential monopsonies in the labour market and cite relevant research.### Assistant: \"Monopsony\" refers to a market structure where there is only one buyer for a par...\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 2:\n",
      "### Human: ¬øCUales son las etapas del desarrollo y en qu√© consisten seg√∫n Piaget?### Assistant: Jean Piaget fue un psic√≥logo suizo que propuso una teor√≠a sobre el desarrollo cognitivo humano que consta de cuatro etapas:\n",
      "\n",
      "Etapa sensoriomotora (0-2 a√±os): Durante esta etapa, el ni√±o aprende a trav√©s d...\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 3:\n",
      "### Human: M√©todo del Perceptr√≥n bicl√°sico: definici√≥n y variantes del m√©todo. ¬øc√≥mo se aplicar√≠a\n",
      "al caso multicl√°sico?, ¬øse podr√≠a utilizar con patrones que s√≥lo son cuadr√°ticamente\n",
      "separables?### Assistant: El m√©todo del Perceptr√≥n bicl√°sico es un algoritmo de aprendizaje autom√°tico que se utiliza...\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ Step 2: Dataset Preparation for OpenAssistant-Guanaco\n",
    "\n",
    "def clean_and_filter_dataset(dataset, max_length=2048, min_length=50):\n",
    "    \"\"\"Clean and filter the dataset for better training quality\"\"\"\n",
    "    \n",
    "    def is_valid_example(example):\n",
    "        text = example['text']\n",
    "        # Filter out very short or very long examples\n",
    "        if len(text) < min_length or len(text) > max_length:\n",
    "            return False\n",
    "        # Ensure it has the expected format\n",
    "        if \"### Human:\" not in text or \"### Assistant:\" not in text:\n",
    "            return False\n",
    "        return True\n",
    "    \n",
    "    # Filter the dataset\n",
    "    filtered_dataset = dataset.filter(is_valid_example)\n",
    "    \n",
    "    print(f\"Original dataset size: {len(dataset)}\")\n",
    "    print(f\"Filtered dataset size: {len(filtered_dataset)}\")\n",
    "    \n",
    "    return filtered_dataset\n",
    "\n",
    "def preprocess_guanaco_text(example):\n",
    "    \"\"\"Preprocess the text to ensure consistent formatting\"\"\"\n",
    "    text = example['text']\n",
    "    \n",
    "    # Ensure proper spacing and formatting\n",
    "    text = text.replace(\"###Human:\", \"### Human:\")\n",
    "    text = text.replace(\"###Assistant:\", \"### Assistant:\")\n",
    "    text = text.replace(\"### Human :\", \"### Human:\")\n",
    "    text = text.replace(\"### Assistant :\", \"### Assistant:\")\n",
    "    \n",
    "    # Add proper ending if missing\n",
    "    if not text.endswith('\\n'):\n",
    "        text += '\\n'\n",
    "    \n",
    "    return {'text': text}\n",
    "\n",
    "print(\"Processing OpenAssistant-Guanaco dataset...\")\n",
    "\n",
    "# Clean and filter the dataset\n",
    "cleaned_dataset = clean_and_filter_dataset(raw_dataset)\n",
    "\n",
    "# Apply preprocessing\n",
    "processed_dataset = cleaned_dataset.map(preprocess_guanaco_text)\n",
    "\n",
    "print(f\"Final processed dataset size: {len(processed_dataset)}\")\n",
    "\n",
    "# Show examples of the processed data\n",
    "print(\"\\n=== Sample Processed Examples ===\")\n",
    "for i in range(min(3, len(processed_dataset))):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    text = processed_dataset[i]['text']\n",
    "    print(text[:300] + (\"...\" if len(text) > 300 else \"\"))\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a4b9d9d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up training dataset...\n",
      "Full dataset size: 7617\n",
      "Limiting dataset to 500 examples for faster training...\n",
      "‚úÖ Dataset reduced from 7617 to 500 examples\n",
      "\n",
      "Final dataset size: 500 examples\n",
      "\n",
      "Sample formatted text:\n",
      "### Human: Can you write a short introduction about the relevance of the term \"monopsony\" in economics? Please use examples related to potential monopsonies in the labour market and cite relevant research.### Assistant: \"Monopsony\" refers to a market structure where there is only one buyer for a particular good or service. In economics, this term is particularly relevant in the labor market, where...\n",
      "\n",
      "üìä Training with 500 examples for optimal training speed\n"
     ]
    }
   ],
   "source": [
    "# Load and process the OpenAssistant-Guanaco dataset\n",
    "print(\"Setting up training dataset...\")\n",
    "\n",
    "# Use the processed dataset\n",
    "hf_dataset = processed_dataset\n",
    "\n",
    "print(f\"Full dataset size: {len(hf_dataset)}\")\n",
    "\n",
    "# Limit dataset to 500 examples for faster training and demonstration\n",
    "MAX_EXAMPLES = 500\n",
    "if len(hf_dataset) > MAX_EXAMPLES:\n",
    "    print(f\"Limiting dataset to {MAX_EXAMPLES} examples for faster training...\")\n",
    "    hf_dataset = hf_dataset.select(range(MAX_EXAMPLES))\n",
    "    print(f\"‚úÖ Dataset reduced from {len(processed_dataset)} to {len(hf_dataset)} examples\")\n",
    "else:\n",
    "    print(f\"Dataset size ({len(hf_dataset)}) is already within the limit\")\n",
    "\n",
    "print(f\"\\nFinal dataset size: {len(hf_dataset)} examples\")\n",
    "print(\"\\nSample formatted text:\")\n",
    "print(hf_dataset[0][\"text\"][:400] + \"...\")\n",
    "\n",
    "print(f\"\\nüìä Training with {len(hf_dataset)} examples for optimal training speed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3128cf96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating train/validation split for limited dataset...\n",
      "Using 20% split (80% train, 20% validation)\n",
      "Training dataset size: 400 examples\n",
      "Evaluation dataset size: 100 examples\n",
      "\n",
      "Training Dataset Statistics:\n",
      "  Examples: 400\n",
      "  Avg text length: 979 characters\n",
      "  Min text length: 57 characters\n",
      "  Max text length: 2030 characters\n",
      "\n",
      "Evaluation Dataset Statistics:\n",
      "  Examples: 100\n",
      "  Avg text length: 1038 characters\n",
      "  Min text length: 134 characters\n",
      "  Max text length: 2013 characters\n",
      "\n",
      "‚úÖ Dataset split complete:\n",
      "  ‚Ä¢ Training: 400 examples\n",
      "  ‚Ä¢ Evaluation: 100 examples\n",
      "  ‚Ä¢ Total: 500 examples\n",
      "\n",
      "Sample evaluation text:\n",
      "### Human: Dame una receta de pan que no contenga gluten.### Assistant: Receta de pan sin gluten.\n",
      "\n",
      "Ingredientes: \n",
      "500 g de harina panificable (sin gluten).\n",
      "10 g de levadura.\n",
      "300 cc de agua.\n",
      "1 cucharadita de sal.\n",
      "\n",
      "Comienza mezclando la harina sin gluten, la levadura y la sal. Luego haz un hueco en el...\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ Create Train/Validation Split for 500-example Dataset\n",
    "\n",
    "print(\"Creating train/validation split for limited dataset...\")\n",
    "\n",
    "# For the 500-example dataset, use 80/20 split\n",
    "test_size = 0.2\n",
    "print(f\"Using {test_size:.0%} split (80% train, 20% validation)\")\n",
    "\n",
    "# Split the dataset into train and validation\n",
    "train_test_split = hf_dataset.train_test_split(test_size=test_size, seed=42)\n",
    "train_dataset = train_test_split['train']\n",
    "eval_dataset = train_test_split['test']\n",
    "\n",
    "print(f\"Training dataset size: {len(train_dataset)} examples\")\n",
    "print(f\"Evaluation dataset size: {len(eval_dataset)} examples\")\n",
    "\n",
    "# Show statistics about the datasets\n",
    "def analyze_dataset(dataset, name):\n",
    "    lengths = [len(example['text']) for example in dataset]\n",
    "    avg_length = sum(lengths) / len(lengths)\n",
    "    max_length = max(lengths)\n",
    "    min_length = min(lengths)\n",
    "    \n",
    "    print(f\"\\n{name} Dataset Statistics:\")\n",
    "    print(f\"  Examples: {len(dataset)}\")\n",
    "    print(f\"  Avg text length: {avg_length:.0f} characters\")\n",
    "    print(f\"  Min text length: {min_length} characters\")\n",
    "    print(f\"  Max text length: {max_length} characters\")\n",
    "\n",
    "analyze_dataset(train_dataset, \"Training\")\n",
    "analyze_dataset(eval_dataset, \"Evaluation\")\n",
    "\n",
    "print(f\"\\n‚úÖ Dataset split complete:\")\n",
    "print(f\"  ‚Ä¢ Training: {len(train_dataset)} examples\")\n",
    "print(f\"  ‚Ä¢ Evaluation: {len(eval_dataset)} examples\")\n",
    "print(f\"  ‚Ä¢ Total: {len(hf_dataset)} examples\")\n",
    "\n",
    "print(f\"\\nSample evaluation text:\")\n",
    "print(eval_dataset[0][\"text\"][:300] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "18c83f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "Model: meta-llama/Llama-2-7b-hf\n",
      "Output directory: ./llama2-claude-lora\n",
      "LoRA rank: 16\n",
      "LoRA alpha: 32\n",
      "Target modules: {'down_proj', 'q_proj', 'gate_proj', 'k_proj', 'v_proj', 'up_proj', 'o_proj'}\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ Step 3: Model Configuration\n",
    "\n",
    "# Model configuration\n",
    "MODEL_NAME = \"meta-llama/Llama-2-7b-hf\"  # Change to \"meta-llama/Llama-2-7b-chat-hf\" for chat version\n",
    "OUTPUT_DIR = \"./llama2-claude-lora\"\n",
    "\n",
    "# Quantization config for memory efficiency\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  # Rank of adaptation\n",
    "    lora_alpha=32,  # LoRA scaling parameter\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "    ],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"LoRA rank: {lora_config.r}\")\n",
    "print(f\"LoRA alpha: {lora_config.lora_alpha}\")\n",
    "print(f\"Target modules: {lora_config.target_modules}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4d48ec99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è  CUDA not available - using CPU-friendly configuration\n",
      "Note: For production training, use a GPU environment\n",
      "CPU Configuration:\n",
      "Model: microsoft/DialoGPT-small\n",
      "Output directory: ./dialogpt-claude-lora-cpu\n",
      "LoRA rank: 8\n",
      "Quantization: Disabled (CPU mode)\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ CPU-Friendly Configuration (for environments without CUDA)\n",
    "\n",
    "# Check if CUDA is available and adjust configuration accordingly\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"‚ö†Ô∏è  CUDA not available - using CPU-friendly configuration\")\n",
    "    print(\"Note: For production training, use a GPU environment\")\n",
    "    \n",
    "    # Use a smaller model for CPU demonstration\n",
    "    MODEL_NAME = \"microsoft/DialoGPT-small\"  # Much smaller model for CPU\n",
    "    OUTPUT_DIR = \"./dialogpt-claude-lora-cpu\"\n",
    "    \n",
    "    # No quantization for CPU\n",
    "    bnb_config = None\n",
    "    \n",
    "    # Simpler LoRA config for smaller model\n",
    "    lora_config = LoraConfig(\n",
    "        r=8,  # Smaller rank for CPU\n",
    "        lora_alpha=16,\n",
    "        target_modules=[\"c_attn\"],  # DialoGPT uses different module names\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "    )\n",
    "    \n",
    "    print(\"CPU Configuration:\")\n",
    "    print(f\"Model: {MODEL_NAME}\")\n",
    "    print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "    print(f\"LoRA rank: {lora_config.r}\")\n",
    "    print(\"Quantization: Disabled (CPU mode)\")\n",
    "\n",
    "else:\n",
    "    print(\"‚úÖ CUDA available - using original GPU configuration\")\n",
    "    # Keep original configuration from previous cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "170852b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n",
      "Tokenizer loaded. Vocab size: 50257\n",
      "Pad token: <|endoftext|>\n",
      "EOS token: <|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Add padding token if it doesn't exist\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Tokenizer loaded. Vocab size: {len(tokenizer)}\")\n",
    "print(f\"Pad token: {tokenizer.pad_token}\")\n",
    "print(f\"EOS token: {tokenizer.eos_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "05925220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing datasets...\n",
      "Tokenized training dataset size: 400\n",
      "Tokenized evaluation dataset size: 100\n",
      "Sample tokenized data keys: dict_keys(['input_ids', 'attention_mask', 'labels'])\n",
      "Sample input_ids length: 194\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the dataset for training\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenize the text data for training\"\"\"\n",
    "    # Tokenize the text\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=False,  # We'll pad dynamically during training\n",
    "        max_length=512,\n",
    "        return_overflowing_tokens=False,\n",
    "    )\n",
    "    \n",
    "    # For causal language modeling, labels are the same as input_ids\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    \n",
    "    return tokenized\n",
    "\n",
    "print(\"Tokenizing datasets...\")\n",
    "\n",
    "# Tokenize training dataset\n",
    "tokenized_train_dataset = train_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names,\n",
    "    desc=\"Tokenizing training dataset\"\n",
    ")\n",
    "\n",
    "# Tokenize evaluation dataset\n",
    "tokenized_eval_dataset = eval_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=eval_dataset.column_names,\n",
    "    desc=\"Tokenizing evaluation dataset\"\n",
    ")\n",
    "\n",
    "print(f\"Tokenized training dataset size: {len(tokenized_train_dataset)}\")\n",
    "print(f\"Tokenized evaluation dataset size: {len(tokenized_eval_dataset)}\")\n",
    "print(f\"Sample tokenized data keys: {tokenized_train_dataset[0].keys()}\")\n",
    "print(f\"Sample input_ids length: {len(tokenized_train_dataset[0]['input_ids'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c211a43e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model...\n",
      "Applying LoRA configuration...\n",
      "Trainable parameters: 294,912\n",
      "Total parameters: 124,734,720\n",
      "Percentage of trainable parameters: 0.24%\n",
      "trainable params: 294,912 || all params: 124,734,720 || trainable%: 0.2364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/tensor/lib/python3.10/site-packages/peft/tuners/lora/layer.py:1264: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load base model\n",
    "print(\"Loading base model...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Apply LoRA\n",
    "print(\"Applying LoRA configuration...\")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print model info\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Percentage of trainable parameters: {100 * trainable_params / total_params:.2f}%\")\n",
    "\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "037d4287",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/r6/b87zwk157sz4fy3q2qlsmrp00000gn/T/ipykernel_40243/1138004128.py:69: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Using standard Trainer with evaluation dataset\n",
      "Training configuration:\n",
      "Epochs: 3\n",
      "Batch size: 1\n",
      "Gradient accumulation: 4\n",
      "Learning rate: 0.0002\n",
      "Max sequence length: 512\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ Step 4: Training Configuration\n",
    "\n",
    "# Adjust training arguments based on available hardware\n",
    "if torch.cuda.is_available():\n",
    "    # GPU configuration\n",
    "    batch_size = 1\n",
    "    fp16_enabled = True\n",
    "    device_pin_memory = True\n",
    "else:\n",
    "    # CPU configuration - more conservative settings\n",
    "    batch_size = 1\n",
    "    fp16_enabled = False  # FP16 not supported on CPU\n",
    "    device_pin_memory = False\n",
    "\n",
    "# Training arguments optimized for 500-example dataset\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=3,  # More epochs for smaller dataset\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=4,  # Smaller accumulation for small dataset\n",
    "    learning_rate=2e-4,  # Standard learning rate for instruction tuning\n",
    "    warmup_ratio=0.1,  # 10% warmup\n",
    "    logging_steps=5,  # More frequent logging for small dataset\n",
    "    eval_steps=20,  # Evaluate every 20 steps (good for ~400 training examples)\n",
    "    save_steps=40,  # Save every 40 steps (multiple of eval_steps=20)\n",
    "    save_total_limit=3,  # Keep 3 best checkpoints\n",
    "    eval_strategy=\"steps\",  # Enable evaluation during training\n",
    "    dataloader_pin_memory=device_pin_memory,\n",
    "    group_by_length=True,  # Group samples by length for efficiency\n",
    "    report_to=\"none\",  # Change to \"wandb\" for experiment tracking\n",
    "    run_name=\"llama2-guanaco-lora-500\",\n",
    "    optim=\"adamw_torch\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    fp16=fp16_enabled,\n",
    "    remove_unused_columns=False,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    # Additional optimizations\n",
    "    max_grad_norm=1.0,  # Gradient clipping\n",
    "    weight_decay=0.01,  # L2 regularization\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "# Note: Newer versions of SFTTrainer may have different parameter names\n",
    "try:\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=train_dataset,  # Use train split\n",
    "        eval_dataset=eval_dataset,    # Use eval split\n",
    "        args=training_args,\n",
    "        max_seq_length=512,\n",
    "        dataset_text_field=\"text\",\n",
    "        packing=False,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    print(\"‚úÖ Using SFTTrainer with evaluation dataset\")\n",
    "except TypeError:\n",
    "    # Fallback for different SFTTrainer API versions or use standard Trainer\n",
    "    from transformers import Trainer, DataCollatorForLanguageModeling\n",
    "    \n",
    "    # Use data collator for language modeling\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False,  # We're doing causal language modeling\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train_dataset,  # Use tokenized train dataset\n",
    "        eval_dataset=tokenized_eval_dataset,    # Use tokenized eval dataset\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "    print(\"‚úÖ Using standard Trainer with evaluation dataset\")\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "print(f\"Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n",
    "print(f\"Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"Max sequence length: 512\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a42c6b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation metrics and monitoring functions ready!\n",
      "- compute_metrics: Custom metric computation for evaluation\n",
      "- plot_training_history: Visualize training progress\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ Evaluation Metrics and Monitoring\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Compute custom metrics for evaluation\"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    # Convert logits to predictions (get the token with highest probability)\n",
    "    predictions = np.argmax(predictions, axis=-1)\n",
    "    \n",
    "    # Flatten arrays for comparison (ignoring padding tokens)\n",
    "    predictions = predictions.flatten()\n",
    "    labels = labels.flatten()\n",
    "    \n",
    "    # Only consider non-padding tokens (assuming -100 is used for padding in labels)\n",
    "    mask = labels != -100\n",
    "    predictions = predictions[mask]\n",
    "    labels = labels[mask]\n",
    "    \n",
    "    # Calculate token-level accuracy\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"token_count\": len(labels)\n",
    "    }\n",
    "\n",
    "def plot_training_history(trainer):\n",
    "    \"\"\"Plot training and evaluation loss\"\"\"\n",
    "    if hasattr(trainer, 'state') and trainer.state.log_history:\n",
    "        logs = trainer.state.log_history\n",
    "        \n",
    "        train_loss = [log['train_loss'] for log in logs if 'train_loss' in log]\n",
    "        eval_loss = [log['eval_loss'] for log in logs if 'eval_loss' in log]\n",
    "        \n",
    "        if train_loss or eval_loss:\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            \n",
    "            if train_loss:\n",
    "                plt.subplot(1, 2, 1)\n",
    "                plt.plot(train_loss, label='Training Loss')\n",
    "                plt.title('Training Loss')\n",
    "                plt.xlabel('Step')\n",
    "                plt.ylabel('Loss')\n",
    "                plt.legend()\n",
    "            \n",
    "            if eval_loss:\n",
    "                plt.subplot(1, 2, 2)\n",
    "                plt.plot(eval_loss, label='Evaluation Loss', color='orange')\n",
    "                plt.title('Evaluation Loss')\n",
    "                plt.xlabel('Step')\n",
    "                plt.ylabel('Loss')\n",
    "                plt.legend()\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(\"No training history available for plotting\")\n",
    "\n",
    "print(\"Evaluation metrics and monitoring functions ready!\")\n",
    "print(\"- compute_metrics: Custom metric computation for evaluation\")\n",
    "print(\"- plot_training_history: Visualize training progress\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a07f6eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting training with 500-example subset of OpenAssistant-Guanaco...\n",
      "This optimized dataset size provides:\n",
      "  ‚Ä¢ Faster training iterations\n",
      "  ‚Ä¢ Reasonable training time\n",
      "  ‚Ä¢ Good model performance for demonstration\n",
      "Evaluation will run every 20 steps to monitor progress.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [300/300 03:18, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>13.716700</td>\n",
       "      <td>13.892773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>13.069500</td>\n",
       "      <td>13.220043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>14.536700</td>\n",
       "      <td>12.606959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>19.245500</td>\n",
       "      <td>12.485124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>12.758100</td>\n",
       "      <td>12.324083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>12.201900</td>\n",
       "      <td>12.092096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>10.020300</td>\n",
       "      <td>11.480578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>11.089800</td>\n",
       "      <td>11.072018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>14.372300</td>\n",
       "      <td>10.702742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>11.278500</td>\n",
       "      <td>10.431932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>10.322300</td>\n",
       "      <td>10.209913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>11.480200</td>\n",
       "      <td>10.078519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>11.900600</td>\n",
       "      <td>10.045635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>11.462700</td>\n",
       "      <td>10.050266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>13.060300</td>\n",
       "      <td>10.049891</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Training completed!\n",
      "Dataset used: OpenAssistant-Guanaco subset (400 training examples)\n",
      "Total training time: 199.33 seconds\n",
      "Final training loss: 13.2918\n",
      "\n",
      "üìä Running final evaluation...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final evaluation results:\n",
      "  eval_loss: 10.0785\n",
      "  eval_runtime: 4.0860\n",
      "  eval_samples_per_second: 24.4740\n",
      "  eval_steps_per_second: 24.4740\n",
      "  epoch: 3.0000\n",
      "\n",
      "üíæ Model saved to ./dialogpt-claude-lora-cpu\n",
      "Your model is now fine-tuned on a curated subset of high-quality instruction-following data!\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ‚úÖ Step 5: Start Training with 500-example Dataset\n",
    "\n",
    "print(\"üöÄ Starting training with 500-example subset of OpenAssistant-Guanaco...\")\n",
    "print(\"This optimized dataset size provides:\")\n",
    "print(\"  ‚Ä¢ Faster training iterations\")\n",
    "print(\"  ‚Ä¢ Reasonable training time\")\n",
    "print(\"  ‚Ä¢ Good model performance for demonstration\")\n",
    "print(\"Evaluation will run every 20 steps to monitor progress.\")\n",
    "\n",
    "# Start training\n",
    "training_result = trainer.train()\n",
    "\n",
    "# Print training summary\n",
    "print(f\"\\n‚úÖ Training completed!\")\n",
    "print(f\"Dataset used: OpenAssistant-Guanaco subset ({len(train_dataset)} training examples)\")\n",
    "print(f\"Total training time: {training_result.metrics.get('train_runtime', 'N/A'):.2f} seconds\" if 'train_runtime' in training_result.metrics else \"\")\n",
    "print(f\"Final training loss: {training_result.metrics.get('train_loss', 'N/A'):.4f}\" if 'train_loss' in training_result.metrics else \"\")\n",
    "\n",
    "# Run final evaluation\n",
    "print(\"\\nüìä Running final evaluation...\")\n",
    "eval_results = trainer.evaluate()\n",
    "print(\"Final evaluation results:\")\n",
    "for key, value in eval_results.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# Save the final model\n",
    "trainer.save_model()\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "print(f\"\\nüíæ Model saved to {OUTPUT_DIR}\")\n",
    "print(\"Your model is now fine-tuned on a curated subset of high-quality instruction-following data!\")\n",
    "\n",
    "# Plot training history if available\n",
    "try:\n",
    "    plot_training_history(trainer)\n",
    "except Exception as e:\n",
    "    print(f\"Could not plot training history: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cd0ca51a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Running comprehensive evaluation...\n",
      "Evaluating on 5 samples...\n",
      "\n",
      "Example 1:\n",
      "Human: –©–æ —Ç–∞–∫–µ –µ–ª–µ–∫—Ç—Ä–∏—á–Ω–∏–π –æ–ø—ñ—Ä? –†–æ–∑–∫–∞–∂–∏ –±—É–¥—å –ª–∞—Å–∫–∞ –¥–µ—Ç–∞–ª—å–Ω–æ...\n",
      "Loss: 7.1896\n",
      "\n",
      "Example 2:\n",
      "Human: crea una lista de 3 titulos novedosos para crear una novela de terror....\n",
      "Loss: 8.4679\n",
      "\n",
      "Example 3:\n",
      "Human: ¬øQu√© es el Counseling?...\n",
      "Loss: 8.6274\n",
      "\n",
      "üìä Evaluation Results:\n",
      "Valid examples processed: 5\n",
      "Average Loss: 10.1788\n",
      "Perplexity: 26339.7109\n",
      "\n",
      "‚úÖ Evaluation complete!\n",
      "This gives us a baseline for model performance on unseen data.\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ Comprehensive Model Evaluation\n",
    "\n",
    "def evaluate_model_on_dataset(model, tokenizer, eval_dataset, num_samples=10):\n",
    "    \"\"\"Evaluate model performance on evaluation dataset\"\"\"\n",
    "    \n",
    "    if num_samples and num_samples < len(eval_dataset):\n",
    "        # Sample random examples for evaluation\n",
    "        import random\n",
    "        indices = random.sample(range(len(eval_dataset)), num_samples)\n",
    "        eval_subset = eval_dataset.select(indices)\n",
    "    else:\n",
    "        eval_subset = eval_dataset\n",
    "    \n",
    "    print(f\"Evaluating on {len(eval_subset)} samples...\")\n",
    "    \n",
    "    total_loss = 0\n",
    "    valid_examples = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    for i, example in enumerate(eval_subset):\n",
    "        try:\n",
    "            # Tokenize the text with proper length handling\n",
    "            inputs = tokenizer(\n",
    "                example[\"text\"], \n",
    "                return_tensors=\"pt\", \n",
    "                truncation=True, \n",
    "                max_length=512\n",
    "            ).to(model.device)\n",
    "            \n",
    "            # Skip if the sequence is too short\n",
    "            if inputs[\"input_ids\"].shape[1] < 10:\n",
    "                continue\n",
    "            \n",
    "            # Get model predictions\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "                loss = outputs.loss\n",
    "                total_loss += loss.item()\n",
    "                valid_examples += 1\n",
    "            \n",
    "            if i < 3:  # Show first 3 examples\n",
    "                print(f\"\\nExample {i+1}:\")\n",
    "                # Show the human question part\n",
    "                text = example['text']\n",
    "                if \"### Human:\" in text:\n",
    "                    human_part = text.split(\"### Human:\")[1].split(\"### Assistant:\")[0].strip()\n",
    "                    print(f\"Human: {human_part[:150]}...\")\n",
    "                print(f\"Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing example {i}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if valid_examples == 0:\n",
    "        print(\"No valid examples processed!\")\n",
    "        return None\n",
    "    \n",
    "    avg_loss = total_loss / valid_examples\n",
    "    perplexity = torch.exp(torch.tensor(avg_loss))\n",
    "    \n",
    "    print(f\"\\nüìä Evaluation Results:\")\n",
    "    print(f\"Valid examples processed: {valid_examples}\")\n",
    "    print(f\"Average Loss: {avg_loss:.4f}\")\n",
    "    print(f\"Perplexity: {perplexity:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        \"avg_loss\": avg_loss,\n",
    "        \"perplexity\": perplexity.item(),\n",
    "        \"valid_examples\": valid_examples\n",
    "    }\n",
    "\n",
    "# Run comprehensive evaluation\n",
    "print(\"üîç Running comprehensive evaluation...\")\n",
    "eval_results = evaluate_model_on_dataset(model, tokenizer, eval_dataset, num_samples=5)\n",
    "\n",
    "print(f\"\\n‚úÖ Evaluation complete!\")\n",
    "print(f\"This gives us a baseline for model performance on unseen data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f00a62b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the fine-tuned model:\n",
      "==================================================\n",
      "\n",
      "üîπ Human: What is machine learning?\n",
      "ü§ñ Assistant: What is an alt?\n",
      "--------------------------------------------------\n",
      "\n",
      "üîπ Human: How do neural networks work?\n",
      "ü§ñ Assistant: How do neural networks work?\n",
      "--------------------------------------------------\n",
      "\n",
      "üîπ Human: Explain the concept of recursion in programming.\n",
      "ü§ñ Assistant: ‚Äú ‚Äù\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ Step 6: Test the Fine-tuned Model\n",
    "\n",
    "def generate_response(prompt: str, max_length: int = 256) -> str:\n",
    "    \"\"\"Generate response using the fine-tuned model\"\"\"\n",
    "    \n",
    "    # Format the prompt\n",
    "    formatted_prompt = f\"### Human: {prompt}\\n\\n### Assistant:\"\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(\n",
    "        formatted_prompt, \n",
    "        return_tensors=\"pt\", \n",
    "        truncation=True, \n",
    "        max_length=512\n",
    "    ).to(model.device)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=inputs[\"input_ids\"].shape[1] + max_length,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            top_p=0.9,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    # Decode\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract only the assistant's response\n",
    "    if \"### Assistant:\" in response:\n",
    "        response = response.split(\"### Assistant:\")[-1].strip()\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Test the model\n",
    "test_prompts = [\n",
    "    \"What is machine learning?\",\n",
    "    \"How do neural networks work?\",\n",
    "    \"Explain the concept of recursion in programming.\"\n",
    "]\n",
    "\n",
    "print(\"Testing the fine-tuned model:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(f\"\\nüîπ Human: {prompt}\")\n",
    "    response = generate_response(prompt)\n",
    "    print(f\"ü§ñ Assistant: {response}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f41af65c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utilities for model loading and deployment are ready!\n",
      "\n",
      "To load the fine-tuned model later:\n",
      "model, tokenizer = load_fine_tuned_model('./llama2-claude-lora')\n",
      "\n",
      "To create a deployment-ready model:\n",
      "deployment_path = save_model_for_deployment('./llama2-claude-lora')\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ Step 7: Model Loading and Deployment Utilities\n",
    "\n",
    "def load_fine_tuned_model(model_path: str):\n",
    "    \"\"\"Load the fine-tuned model for inference\"\"\"\n",
    "    from peft import PeftModel\n",
    "    \n",
    "    # Load base model\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16,\n",
    "    )\n",
    "    \n",
    "    # Load LoRA weights\n",
    "    model = PeftModel.from_pretrained(base_model, model_path)\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "def save_model_for_deployment(output_dir: str):\n",
    "    \"\"\"Save model in a format suitable for deployment\"\"\"\n",
    "    \n",
    "    # Merge LoRA weights with base model for deployment\n",
    "    merged_model = model.merge_and_unload()\n",
    "    \n",
    "    # Save merged model\n",
    "    deployment_dir = f\"{output_dir}_merged\"\n",
    "    merged_model.save_pretrained(deployment_dir)\n",
    "    tokenizer.save_pretrained(deployment_dir)\n",
    "    \n",
    "    print(f\"Merged model saved to {deployment_dir}\")\n",
    "    print(\"This merged model can be used without PEFT dependencies.\")\n",
    "    \n",
    "    return deployment_dir\n",
    "\n",
    "# Example usage (uncomment to use)\n",
    "# deployment_path = save_model_for_deployment(OUTPUT_DIR)\n",
    "\n",
    "print(\"Utilities for model loading and deployment are ready!\")\n",
    "print(\"\\nTo load the fine-tuned model later:\")\n",
    "print(\"model, tokenizer = load_fine_tuned_model('./llama2-claude-lora')\")\n",
    "print(\"\\nTo create a deployment-ready model:\")\n",
    "print(\"deployment_path = save_model_for_deployment('./llama2-claude-lora')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30c376f",
   "metadata": {},
   "source": [
    "## üîß Advanced Configuration & Tips\n",
    "\n",
    "### Memory Optimization\n",
    "- **4-bit Quantization**: Reduces memory usage by ~75%\n",
    "- **Gradient Checkpointing**: Enable with `gradient_checkpointing=True`\n",
    "- **DeepSpeed**: For multi-GPU training, add DeepSpeed configuration\n",
    "\n",
    "### LoRA Parameters Tuning\n",
    "- **Rank (r)**: Higher rank = more parameters but better adaptation (8-64)\n",
    "- **Alpha**: Usually 2√órank for optimal performance\n",
    "- **Target Modules**: Include attention and MLP layers for best results\n",
    "\n",
    "### Dataset Scaling\n",
    "For production use:\n",
    "1. Use datasets with 10K+ examples\n",
    "2. Implement proper train/validation splits\n",
    "3. Add data filtering and quality checks\n",
    "4. Consider multi-turn conversation handling\n",
    "\n",
    "### Training Optimizations\n",
    "```python\n",
    "# Example production training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./llama2-claude-production\",\n",
    "    num_train_epochs=1,  # Usually 1-3 epochs for instruction tuning\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,\n",
    "    max_grad_norm=1.0,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    logging_steps=25,\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    fp16=True,\n",
    "    dataloader_num_workers=4,\n",
    "    remove_unused_columns=False,\n",
    "    report_to=\"wandb\",  # For experiment tracking\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d205964",
   "metadata": {},
   "source": [
    "## üìä Evaluation Setup Summary\n",
    "\n",
    "This notebook now includes comprehensive evaluation capabilities:\n",
    "\n",
    "### üîÑ Training Evaluation\n",
    "- **Train/Validation Split**: 80/20 split with additional evaluation samples\n",
    "- **Evaluation Strategy**: Evaluation runs every few training steps\n",
    "- **Metrics Tracked**: Loss, perplexity, and token-level accuracy\n",
    "- **Best Model Selection**: Automatically saves the best model based on evaluation loss\n",
    "\n",
    "### üìà Evaluation Metrics\n",
    "- **Perplexity**: Measures how well the model predicts the next token (lower is better)\n",
    "- **Loss**: Cross-entropy loss on the evaluation set\n",
    "- **Token Accuracy**: Percentage of correctly predicted tokens\n",
    "\n",
    "### üéØ Evaluation Best Practices\n",
    "1. **Diverse Evaluation Data**: Include various types of questions and responses\n",
    "2. **Regular Monitoring**: Evaluate frequently to catch overfitting early\n",
    "3. **Multiple Metrics**: Don't rely on loss alone - use task-specific metrics\n",
    "4. **Human Evaluation**: Supplement automated metrics with human assessment\n",
    "\n",
    "### üè≠ Production Evaluation\n",
    "For production systems, consider:\n",
    "- **Larger Evaluation Sets**: 1000+ examples covering edge cases\n",
    "- **Domain-Specific Metrics**: BLEU, ROUGE, or custom task metrics\n",
    "- **A/B Testing**: Compare model versions in real deployment\n",
    "- **Continuous Monitoring**: Track performance drift over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68418fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook llm_train.ipynb to python\n",
      "[NbConvertApp] Writing 29428 bytes to llm_training_script.py\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to python llm_train.ipynb --output llm_training_script\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
